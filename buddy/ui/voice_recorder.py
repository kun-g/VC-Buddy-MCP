import os
import tempfile
import threading
import wave
from typing import Optional, Callable
import pyaudio
import openai
from PySide6.QtCore import QObject, Signal, QThread
from PySide6.QtWidgets import QPushButton, QMessageBox


class VoiceRecorder(QObject):
    """è¯­éŸ³å½•åˆ¶å™¨"""
    
    # ä¿¡å·å®šä¹‰
    recording_started = Signal()
    recording_stopped = Signal()
    transcription_ready = Signal(str)  # è½¬å†™ç»“æœ
    error_occurred = Signal(str)  # é”™è¯¯ä¿¡æ¯
    
    def __init__(self):
        super().__init__()
        self.is_recording = False
        self.audio_data = []
        self.stream = None
        self.audio = None
        
        # éŸ³é¢‘å‚æ•°
        self.chunk = 1024
        self.format = pyaudio.paInt16
        self.channels = 1
        self.rate = 16000
        
        # OpenAI å®¢æˆ·ç«¯
        self.openai_client = None
        self._init_openai_client()
    
    def _init_openai_client(self):
        """åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯"""
        api_key = os.getenv('OPENAI_API_KEY')
        if api_key:
            self.openai_client = openai.OpenAI(api_key=api_key)
        else:
            print("Warning: OPENAI_API_KEY not found in environment variables")
    
    def start_recording(self):
        """å¼€å§‹å½•éŸ³"""
        if self.is_recording:
            return
        
        try:
            self.audio = pyaudio.PyAudio()
            self.stream = self.audio.open(
                format=self.format,
                channels=self.channels,
                rate=self.rate,
                input=True,
                frames_per_buffer=self.chunk
            )
            
            self.is_recording = True
            self.audio_data = []
            
            # åœ¨æ–°çº¿ç¨‹ä¸­å½•éŸ³
            self.recording_thread = threading.Thread(target=self._record_audio)
            self.recording_thread.start()
            
            self.recording_started.emit()
            
        except Exception as e:
            self.error_occurred.emit(f"å½•éŸ³å¯åŠ¨å¤±è´¥: {str(e)}")
    
    def stop_recording(self):
        """åœæ­¢å½•éŸ³"""
        if not self.is_recording:
            return
        
        self.is_recording = False
        
        # ç­‰å¾…å½•éŸ³çº¿ç¨‹ç»“æŸ
        if hasattr(self, 'recording_thread'):
            self.recording_thread.join()
        
        # æ¸…ç†èµ„æº
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
        if self.audio:
            self.audio.terminate()
        
        self.recording_stopped.emit()
        
        # å¼€å§‹è½¬å†™
        if self.audio_data:
            self._start_transcription()
    
    def _record_audio(self):
        """å½•éŸ³çº¿ç¨‹å‡½æ•°"""
        while self.is_recording:
            try:
                data = self.stream.read(self.chunk, exception_on_overflow=False)
                self.audio_data.append(data)
            except Exception as e:
                self.error_occurred.emit(f"å½•éŸ³è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                break
    
    def _start_transcription(self):
        """å¼€å§‹è½¬å†™éŸ³é¢‘"""
        if not self.openai_client:
            self.error_occurred.emit("OpenAI API æœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œè¯­éŸ³è½¬å†™")
            return
        
        # åœ¨æ–°çº¿ç¨‹ä¸­è¿›è¡Œè½¬å†™
        transcription_thread = threading.Thread(target=self._transcribe_audio)
        transcription_thread.start()
    
    def _transcribe_audio(self):
        """è½¬å†™éŸ³é¢‘"""
        try:
            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_filename = temp_file.name
                
                # å†™å…¥ WAV æ–‡ä»¶
                with wave.open(temp_filename, 'wb') as wf:
                    wf.setnchannels(self.channels)
                    wf.setsampwidth(self.audio.get_sample_size(self.format))
                    wf.setframerate(self.rate)
                    wf.writeframes(b''.join(self.audio_data))
            
            # è°ƒç”¨ OpenAI Whisper API
            with open(temp_filename, 'rb') as audio_file:
                transcript = self.openai_client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="zh"  # æŒ‡å®šä¸­æ–‡
                )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_filename)
            
            # å‘é€è½¬å†™ç»“æœ
            self.transcription_ready.emit(transcript.text)
            
        except Exception as e:
            self.error_occurred.emit(f"è¯­éŸ³è½¬å†™å¤±è´¥: {str(e)}")


class VoiceButton(QPushButton):
    """è¯­éŸ³å½•åˆ¶æŒ‰é’®"""
    
    def __init__(self, parent=None):
        super().__init__("ğŸ¤", parent)
        self.setToolTip("ç‚¹å‡»å¼€å§‹å½•éŸ³ï¼Œå†æ¬¡ç‚¹å‡»åœæ­¢å½•éŸ³")
        self.setFixedSize(40, 40)
        
        # åˆ›å»ºå½•éŸ³å™¨
        self.recorder = VoiceRecorder()
        
        # è¿æ¥ä¿¡å·
        self.recorder.recording_started.connect(self._on_recording_started)
        self.recorder.recording_stopped.connect(self._on_recording_stopped)
        self.recorder.error_occurred.connect(self._on_error)
        
        # è¿æ¥æŒ‰é’®ç‚¹å‡»äº‹ä»¶
        self.clicked.connect(self._toggle_recording)
        
        # æ ·å¼
        self.setStyleSheet("""
            QPushButton {
                background-color: #f0f0f0;
                border: 2px solid #ccc;
                border-radius: 20px;
                font-size: 16px;
            }
            QPushButton:hover {
                background-color: #e0e0e0;
                border-color: #999;
            }
            QPushButton:pressed {
                background-color: #d0d0d0;
            }
            QPushButton[recording="true"] {
                background-color: #ff4444;
                border-color: #cc0000;
                color: white;
            }
        """)
    
    def _toggle_recording(self):
        """åˆ‡æ¢å½•éŸ³çŠ¶æ€"""
        if self.recorder.is_recording:
            self.recorder.stop_recording()
        else:
            self.recorder.start_recording()
    
    def _on_recording_started(self):
        """å½•éŸ³å¼€å§‹æ—¶çš„å¤„ç†"""
        self.setText("â¹ï¸")
        self.setToolTip("ç‚¹å‡»åœæ­¢å½•éŸ³")
        self.setProperty("recording", "true")
        self.style().unpolish(self)
        self.style().polish(self)
    
    def _on_recording_stopped(self):
        """å½•éŸ³åœæ­¢æ—¶çš„å¤„ç†"""
        self.setText("ğŸ¤")
        self.setToolTip("ç‚¹å‡»å¼€å§‹å½•éŸ³ï¼Œå†æ¬¡ç‚¹å‡»åœæ­¢å½•éŸ³")
        self.setProperty("recording", "false")
        self.style().unpolish(self)
        self.style().polish(self)
    
    def _on_error(self, error_message: str):
        """é”™è¯¯å¤„ç†"""
        QMessageBox.warning(self, "è¯­éŸ³å½•åˆ¶é”™è¯¯", error_message)
        self._on_recording_stopped()  # é‡ç½®æŒ‰é’®çŠ¶æ€
    
    def connect_transcription_ready(self, callback: Callable[[str], None]):
        """è¿æ¥è½¬å†™å®Œæˆçš„å›è°ƒå‡½æ•°"""
        self.recorder.transcription_ready.connect(callback) 